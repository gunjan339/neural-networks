{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "obvious-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interracial-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt('train_X.csv', delimiter = ',').T\n",
    "Y_train = np.loadtxt('train_label.csv', delimiter = ',').T\n",
    "\n",
    "X_test = np.loadtxt('test_X.csv', delimiter = ',').T\n",
    "Y_test = np.loadtxt('test_label.csv', delimiter = ',').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of X_train :\", X_train.shape)\n",
    "print(\"shape of Y_train :\", Y_train.shape)\n",
    "print(\"shape of X_test :\", X_test.shape)\n",
    "print(\"shape of Y_test :\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randrange(0, X_train.shape[1])\n",
    "plt.imshow(X_train[:, index].reshape(28, 28), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "centered-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX/np.sum(expX, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "functional-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_tanh(x):\n",
    "    return (1 - np.power(np.tanh(x), 2))\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.array(x > 0, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prepared-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    w1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    \n",
    "    w2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        \"w1\" : w1,\n",
    "        \"b1\" : b1,\n",
    "        \"w2\" : w2,\n",
    "        \"b2\" : b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "voluntary-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, parameters):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    z1 = np.dot(w1, x) + b1\n",
    "    a1 = tanh(z1)\n",
    "    \n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "    \n",
    "    forward_cache = {\n",
    "        \"z1\" : z1,\n",
    "        \"a1\" : a1,\n",
    "        \"z2\" : z2,\n",
    "        \"a2\" : a2\n",
    "    }\n",
    "    \n",
    "    return forward_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "legendary-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(a2, y):\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    cost = -(1/m)*np.sum(y*np.log(a2))\n",
    "    \n",
    "    #cost = -(1/m)*np.sum(np.sum(y*np.log(a2, 0), 1))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weird-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(x, y, parameters, forward_cache):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    a1 = forward_cache['a1']\n",
    "    a2 = forward_cache['a2']\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    \n",
    "    dz2 = (a2 - y)\n",
    "    dw2 = (1/m)*np.dot(dz2, a1.T)\n",
    "    db2 = (1/m)*np.sum(dz2, axis = 1, keepdims = True)\n",
    "    \n",
    "    dz1 = (1/m)*np.dot(w2.T, dz2)*derivative_tanh(a1)\n",
    "    dw1 = (1/m)*np.dot(dz1, x.T)\n",
    "    db1 = (1/m)*np.sum(dz1, axis = 1, keepdims = True)\n",
    "    \n",
    "    gradients = {\n",
    "        \"dw1\" : dw1,\n",
    "        \"db1\" : db1,\n",
    "        \"dw2\" : dw2,\n",
    "        \"db2\" : db2\n",
    "    }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "written-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dw1 = gradients['dw1']\n",
    "    db1 = gradients['db1']\n",
    "    dw2 = gradients['dw2']\n",
    "    db2 = gradients['db2']\n",
    "    \n",
    "    w1 = w1 - learning_rate*dw1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    w2 = w2 - learning_rate*dw2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\n",
    "        \"w1\" : w1,\n",
    "        \"b1\" : b1,\n",
    "        \"w2\" : w2,\n",
    "        \"b2\" : b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dietary-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y, n_h, learning_rate, iterations):\n",
    "    \n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "    \n",
    "    cost_list = []\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        forward_cache = forward_prop(x, parameters)\n",
    "        \n",
    "        cost = cost_function(forward_cache['a2'], y)\n",
    "        \n",
    "        gradients = backward_prop(x, y, parameters, forward_cache)\n",
    "        \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        if(i%(iterations/10) == 0):\n",
    "            print(\"Cost after\", i, \"iterations is :\", cost)\n",
    "        \n",
    "    return parameters, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "integral-credit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations is : 2.3313655955741495\n",
      "Cost after 10 iterations is : 1.1898558431587547\n",
      "Cost after 20 iterations is : 0.8381312468226737\n",
      "Cost after 30 iterations is : 0.6708050703858767\n",
      "Cost after 40 iterations is : 0.5700502736899008\n",
      "Cost after 50 iterations is : 0.5008112736928935\n",
      "Cost after 60 iterations is : 0.4492162052799746\n",
      "Cost after 70 iterations is : 0.40864940565550045\n",
      "Cost after 80 iterations is : 0.3755287671352622\n",
      "Cost after 90 iterations is : 0.34773012438510514\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "n_h = 1000\n",
    "learning_rate = 0.02\n",
    "Parameters, Cost_list = model(X_train, Y_train, n_h = n_h, learning_rate = learning_rate, iterations = iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, iterations)\n",
    "plt.plot(t, Cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "french-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(inp, labels, parameters):\n",
    "    forward_cache = forward_prop(inp, parameters)\n",
    "    a_out = forward_cache['a2']   \n",
    "    a_out = np.argmax(a_out, 0)  \n",
    "    labels = np.argmax(labels, 0)\n",
    "    acc = np.mean(a_out == labels)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mexican-jamaica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Dataset 95.39999999999999 %\n",
      "Accuracy of Test Dataset 84.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Train Dataset\", accuracy(X_train, Y_train, Parameters), \"%\")\n",
    "print(\"Accuracy of Test Dataset\", round(accuracy(X_test, Y_test, Parameters), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(random.randrange(0,X_test.shape[1]))\n",
    "plt.imshow(X_test[:, idx].reshape((28,28)),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "cache = forward_prop(X_test[:, idx].reshape(X_test[:, idx].shape[0], 1), Parameters)\n",
    "a_pred = cache['a2']  \n",
    "a_pred = np.argmax(a_pred, 0)\n",
    "\n",
    "print(\"Our model says it is :\", a_pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
